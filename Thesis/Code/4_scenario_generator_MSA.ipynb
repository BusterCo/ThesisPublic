{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.GeoSpatialEncoder import GeoSpatialEncoder\n",
    "from custom.PC_Class import PC\n",
    "from custom.Pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from custom.DataCreator import InstanceFileWriter, Scenario\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names, but KMeans was fitted with feature names\")\n",
    "\n",
    "\n",
    "direct = os.getcwd()\n",
    "file_path_cleaned = direct + \"////data////vos_input_data////MultiHubData3_cleaned.csv\" \n",
    "file_path_training = direct + \"////data////vos_input_data////MultiHubData3_training.csv\" \n",
    "\n",
    "def load_data(file_path):\n",
    "    datetime_cols = ['CREATIONDATETIME', 'LAAD_DATETIME_VAN', 'LAAD_DATETIME_TOT', 'LOS_DATETIME_VAN', 'LOS_DATETIME_TOT', '15CREATIONDATETIME']\n",
    "    total_rows = sum(1 for row in open(file_path, 'r', encoding='utf-8'))\n",
    "    chunk_size = 10000  \n",
    "    tqdm.pandas(desc=\"Reading CSV\")\n",
    "    chunks = pd.read_csv(file_path, chunksize=chunk_size, iterator=True, index_col = 0, parse_dates=datetime_cols)\n",
    "\n",
    "    df_orders = pd.concat(tqdm(chunks, total=total_rows//chunk_size))\n",
    "    print(\"Lenght of input data:\", str(len(df_orders)))\n",
    "    return df_orders\n",
    "\n",
    "df_orders = load_data(file_path_cleaned)\n",
    "df_orders_training = load_data(file_path_training)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PC_obj = PC()\n",
    "print(\"PC object created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, df_orders, pc=None):\n",
    "        if pc is None:\n",
    "            pc = PC()\n",
    "        else:\n",
    "            self.pc = pc\n",
    "        self.pipelines_list = {}\n",
    "        self.GSE_list = {}\n",
    "        self.df_orders = df_orders\n",
    "\n",
    "    def add_pipeline(self, pipeline):\n",
    "        self.pipelines_list[pipeline.company] = pipeline\n",
    "        self.pipelines_list[pipeline.company].get_X_and_Y()\n",
    "        self.pipelines_list[pipeline.company].train_classifier()\n",
    "        mode = pipeline.mode\n",
    "        if mode == \"parallel\":\n",
    "            self.pipelines_list[pipeline.company].train_parallel_regressor()\n",
    "        elif mode == \"dirty\":\n",
    "            self.pipelines_list[pipeline.company].train_dirty_regressor()\n",
    "        elif mode == \"clean\":\n",
    "            self.pipelines_list[pipeline.company].train_clean_regressor()\n",
    "        print(f\"Added pipeline for company {pipeline.company}\")\n",
    "\n",
    "    def add_GSE(self, company, n_clusters):\n",
    "        df_temp = self.df_orders[self.df_orders['OPDRACHTGEVERNAAM'] == company]\n",
    "        self.GSE_list[company] = GeoSpatialEncoder(self.pc)\n",
    "        self.GSE_list[company].set_verbose(False)\n",
    "        self.GSE_list[company].set_input_df(df_temp)\n",
    "        self.GSE_list[company].clean_input_df()\n",
    "        self.GSE_list[company].train_kmeans(n_clusters, 'SHIPMENT_COUNT')\n",
    "        self.GSE_list[company].compute_distribution()\n",
    "        self.GSE_list[company].distribution_per_cluster()\n",
    "        print(f\"Added GSE for company {company} with {n_clusters} clusters\")\n",
    "\n",
    "    def return_delivery_region(self, company, CPC):\n",
    "        if company in self.GSE_list:\n",
    "            return self.GSE_list[company].return_cluster(CPC)\n",
    "        \n",
    "    \n",
    "    def predict_order_row(self, row):\n",
    "        # Generate input row\n",
    "        row_df = row.to_frame().T\n",
    "        company = row[\"OPDRACHTGEVERNAAM\"]\n",
    "        if company not in self.pipelines_list:\n",
    "            print(f\"Company {company} not found in pipelines list\")\n",
    "            return\n",
    "        pipeline = self.pipelines_list[company]\n",
    "        return pipeline.predict_demands_with_correction(row_df, softmax=False)\n",
    "    \n",
    "\n",
    "    def predict_mean_order_row(self, row):\n",
    "        # Generate input row\n",
    "        row_df = row.to_frame().T\n",
    "        company = row[\"OPDRACHTGEVERNAAM\"]\n",
    "        if company not in self.pipelines_list:\n",
    "            print(f\"Company {company} not found in pipelines list\")\n",
    "            return\n",
    "        pipeline = self.pipelines_list[company]\n",
    "        demand = row[\"PALLETPLAATSEN\"]\n",
    "        # Creating a dictionary to hold your data\n",
    "        data = {}\n",
    "        for i in range(len(self.GSE_list[company].cluster_distribution)):\n",
    "            region_name = 'REGION_' + str(i)\n",
    "            data[region_name] = round(self.GSE_list[company].cluster_distribution[i] * demand, 1)\n",
    "\n",
    "        # Creating a DataFrame from the dictionary\n",
    "        df = pd.DataFrame([data])\n",
    "        return df\n",
    "\n",
    "    def generate_predicted_orders(self, known_orders_a, company):\n",
    "        df_predicted = pd.DataFrame()\n",
    "        c_known_orders_a = condense_a_orders(known_orders_a)\n",
    "        c_known_orders_a = c_known_orders_a[c_known_orders_a[\"OPDRACHTGEVERNAAM\"] == company]\n",
    "        for _ ,row in c_known_orders_a.iterrows():\n",
    "            if company not in self.pipelines_list:\n",
    "                # print(f\"Company {company} not found in pipelines list\")\n",
    "                continue\n",
    "            pred = self.predict_order_row(row)\n",
    "            pred['MATCHING_KEY'] = row['MATCHING_KEY']\n",
    "            pred[\"LAAD_CPC\"] = row[\"LAAD_CPC\"]\n",
    "            df_predicted = pd.concat([df_predicted,pred])\n",
    "        return df_predicted  \n",
    "    \n",
    "    def generate_mean_orders(self, known_orders_a, company):\n",
    "        df_predicted = pd.DataFrame()\n",
    "        c_known_orders_a = condense_a_orders(known_orders_a)\n",
    "        c_known_orders_a = c_known_orders_a[c_known_orders_a[\"OPDRACHTGEVERNAAM\"] == company]\n",
    "        for _ ,row in c_known_orders_a.iterrows():\n",
    "            if company not in self.pipelines_list:\n",
    "                # print(f\"Company {company} not found in pipelines list\")\n",
    "                continue\n",
    "            pred = self.predict_mean_order_row(row)\n",
    "            pred['MATCHING_KEY'] = row['MATCHING_KEY']\n",
    "            pred[\"LAAD_CPC\"] = row[\"LAAD_CPC\"]\n",
    "            df_predicted = pd.concat([df_predicted,pred])\n",
    "        return df_predicted\n",
    "    \n",
    "    def get_satisfied_demand_for_company(self, df_d, company):\n",
    "        n_regions = self.GSE_list[company].return_n_clusters()\n",
    "        df_d = df_d[df_d[\"OPDRACHTGEVERNAAM\"] == company]\n",
    "        df_d = df_d[df_d[\"AFHCODE\"] == 'd']\n",
    "        df_d = df_d[df_d[\"HAS_PICKUP_TRIP\"] == False]\n",
    "        df_d[\"MATCHING_KEY\"] = df_d['OPDRACHTGEVERID'].astype(str) + '_' + df_d['LAAD_DATETIME_VAN'].dt.date.astype(str) + '_' + df_d['LAADPC'].astype(str)\n",
    "        df_d['REGION'] = df_d.apply(lambda row: self.return_delivery_region(row['OPDRACHTGEVERNAAM'], row['LOS_CPC']), axis=1)\n",
    "        df_d_aggregated = df_d.groupby(['MATCHING_KEY', 'REGION']).agg({\n",
    "                'AFHCODE': 'count',\n",
    "                'OPDRACHTGEVERNAAM': 'first',\n",
    "                'PALLETPLAATSEN': 'sum',\n",
    "            }).reset_index().rename(columns={'AFHCODE': 'AANTALORDERS'})\n",
    "        pivot_df = df_d_aggregated.pivot_table(index='MATCHING_KEY', columns='REGION', values='PALLETPLAATSEN', aggfunc='sum', fill_value=0)\n",
    "        pivot_df.columns = ['REGION_' + str(col) for col in pivot_df.columns]\n",
    "        for i in range(n_regions):  \n",
    "            column_name = 'REGION_' + str(i)\n",
    "            if column_name not in pivot_df.columns:\n",
    "                pivot_df[column_name] = 0  # Add missing region columns\n",
    "        pivot_df = pivot_df[sorted(pivot_df.columns, key=lambda x: int(x.split('_')[1]))]\n",
    "        pivot_df.reset_index(inplace=True)\n",
    "        return pivot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "with open('final_pipeline_parameters.json', \"r\") as f:\n",
    "    parameters = json.load(f)\n",
    "\n",
    "predictor = Predictor(df_orders_training, pc= PC_obj)\n",
    "\n",
    "for company in parameters:\n",
    "    print(company)\n",
    "    # Create GSE\n",
    "    predictor.add_GSE(company, parameters[company][\"n_clusters\"])\n",
    "\n",
    "    # Create pipeline (self, classifier, regressor, mode, df, company)\n",
    "    classifier = xgb.XGBClassifier(random_state = 42,\n",
    "                                   n_estimators=parameters[company]['class_parameters']['n_estimators'],\n",
    "                                   max_depth=parameters[company]['class_parameters']['max_depth'],\n",
    "                                   learning_rate=parameters[company]['class_parameters']['learning_rate'])\n",
    "    regressor = xgb.XGBRegressor(\n",
    "                    random_state = 42,\n",
    "                    n_estimators = parameters[company]['regressor_parameters']['n_estimators'],\n",
    "                    max_depth = parameters[company]['regressor_parameters']['max_depth'],\n",
    "                    learning_rate = parameters[company]['regressor_parameters']['learning_rate']\n",
    "                )\n",
    "    df = predictor.GSE_list[company].condense_orders()\n",
    "    predictor.add_pipeline(Pipeline(classifier, regressor, \"parallel\", df, company))\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDataByTime(df, delivery_day, planning_day, split_time):\n",
    "    date = pd.Timestamp(delivery_day)\n",
    "    split_time = pd.Timestamp(split_time)\n",
    "    date_minus_1 = pd.Timestamp(planning_day)\n",
    "    df_orders_filtered = df[df['LOS_DATETIME_VAN'].dt.date == pd.Timestamp(date).date()] # orders delivered on date\n",
    "    split_datetime = pd.Timestamp(date_minus_1).replace(hour=split_time.hour, minute=split_time.minute)\n",
    "\n",
    "    df_orders_filtered_pre = df_orders_filtered[df_orders_filtered['15CREATIONDATETIME'] < split_datetime]\n",
    "    df_orders_filtered_post = df_orders_filtered[df_orders_filtered['15CREATIONDATETIME'] >= split_datetime]\n",
    "    return df_orders_filtered_pre, df_orders_filtered_post\n",
    "\n",
    "def SplitDFtoAandDbyTime(df, delivery_day, planning_day, planning_day_split_time):\n",
    "    delivery_day = pd.Timestamp(delivery_day)\n",
    "    planning_day = pd.Timestamp(planning_day)\n",
    "    split_time = pd.Timestamp(planning_day_split_time)\n",
    "    split_datetime = pd.Timestamp(planning_day).replace(hour=split_time.hour, minute=split_time.minute)\n",
    "    df_a = df[(df['LOS_DATETIME_VAN'].dt.date == planning_day.date()) & (df['AFHCODE'] == 'a')]\n",
    "    df_d = df[(df['LOS_DATETIME_VAN'].dt.date == delivery_day.date()) & (df['AFHCODE'] == 'd')]\n",
    "\n",
    "    df_a_pre = df_a[df_a['15CREATIONDATETIME'] < split_datetime]\n",
    "    df_d_pre = df_d[df_d['15CREATIONDATETIME'] < split_datetime]\n",
    "    return df_a_pre, df_d_pre\n",
    "\n",
    "def condense_a_orders(df_a):\n",
    "    df_a = df_a[df_a[\"AFHCODE\"] == 'a']\n",
    "    df_a[\"MATCHING_KEY\"] = df_a['OPDRACHTGEVERID'].astype(str) + '_' + df_a['LAAD_DATETIME_VAN'].dt.date.astype(str) + '_' + df_a['LAADPC'].astype(str)\n",
    "    # Step 3: Aggregate data for AFHCODE == 'a'\n",
    "    df_a_aggregated = df_a.groupby('MATCHING_KEY').agg({\n",
    "            'CREATIONDATETIME': 'first',\n",
    "            'AFHCODE': 'count',\n",
    "            'OPDRACHTGEVERNAAM': 'first',\n",
    "            'OPDRACHTGEVERID': 'first',\n",
    "            'PALLETPLAATSEN': 'sum',\n",
    "            'LAADPC': 'first',\n",
    "            'LAAD_CPC': 'first',\n",
    "            'LAAD_DATETIME_VAN': 'first'\n",
    "        }).reset_index().rename(columns={'AFHCODE': 'AANTALORDERS'})\n",
    "    df_a_aggregated['dayofweekcreation'] = pd.to_datetime(df_a_aggregated['CREATIONDATETIME']).dt.dayofweek\n",
    "    df_a_aggregated['weeknr'] = df_a_aggregated[\"CREATIONDATETIME\"].dt.strftime(\"%V\")\n",
    "    return df_a_aggregated\n",
    "\n",
    "def condense_d_orders(df_d, GSE):\n",
    "    df_d = df_d[df_d[\"AFHCODE\"] == 'd']\n",
    "    df_d[\"MATCHING_KEY\"] = df_d['OPDRACHTGEVERID'].astype(str) + '_' + df_d['LAAD_DATETIME_VAN'].dt.date.astype(str) + '_' + df_d['LAADPC'].astype(str)\n",
    "    df_d['REGION'] = df_d['LAADPC'].apply(lambda x: GSE.get_region(x))\n",
    "    # Step 3: Aggregate data for AFHCODE == 'a'\n",
    "    df_d_aggregated = df_d.groupby('MATCHING_KEY').agg({\n",
    "            'CREATIONDATETIME': 'first',\n",
    "            'AFHCODE': 'count',\n",
    "            'OPDRACHTGEVERNAAM': 'first',\n",
    "            'OPDRACHTGEVERID': 'first',\n",
    "            'PALLETPLAATSEN': 'sum',\n",
    "            'LAADPC': 'first',\n",
    "            'LAAD_DATETIME_VAN': 'first'\n",
    "        }).reset_index().rename(columns={'AFHCODE': 'AANTALORDERS'})\n",
    "    return df_d_aggregated\n",
    "\n",
    "def compensate_predicted_volumesold(df_predicted, df_satisfied, known_orders_a):\n",
    "    # Set 'MATCHING_KEY' as the index for both DataFrames if not already\n",
    "    if df_predicted.index.name != 'MATCHING_KEY':\n",
    "        df_predicted = df_predicted.set_index('MATCHING_KEY')\n",
    "    if df_satisfied.index.name != 'MATCHING_KEY':\n",
    "        df_satisfied = df_satisfied.set_index('MATCHING_KEY')\n",
    "    \n",
    "    pred_dict = df_predicted.to_dict(orient='index')\n",
    "    satisfied_dict = df_satisfied.to_dict(orient='index')\n",
    "    known_orders_a_c = condense_a_orders(known_orders_a)\n",
    "    known_orders_a_c.set_index('MATCHING_KEY', inplace=True)\n",
    "    known_a_dict = known_orders_a_c.to_dict(orient='index')\n",
    "    for matchingkey in pred_dict:\n",
    "        volume_predicted = sum([pred_dict[matchingkey][region] for region in pred_dict[matchingkey] if region != 'LAAD_CPC'])\n",
    "        volume_satisfied = 0\n",
    "        if matchingkey in satisfied_dict:\n",
    "            volume_satisfied = sum([satisfied_dict[matchingkey][region] for region in satisfied_dict[matchingkey] if region != 'LAAD_CPC'])\n",
    "        volume_given = 0\n",
    "        if matchingkey in known_a_dict:\n",
    "            volume_given = known_a_dict[matchingkey]['PALLETPLAATSEN']\n",
    "        # if  the volume sattisfied is bigger than the amount predicted, drop prediction rows\n",
    "        if volume_satisfied >= volume_predicted:\n",
    "            df_predicted.drop(matchingkey, inplace=True)      \n",
    "\n",
    "    # Store the 'LAAD_CPC' from df_predicted before reindexing\n",
    "    laad_cpc = df_predicted['LAAD_CPC']\n",
    "\n",
    "    # Ensure both DataFrames have only the columns that are in df_predicted for region calculations\n",
    "    common_columns = df_predicted.columns.intersection(df_satisfied.columns)\n",
    "    # Reindex both dataframes to match df_predicted's rows and common columns, fill missing data with zeros\n",
    "    df_predicted = df_predicted.reindex(columns=common_columns, fill_value=0)\n",
    "    df_satisfied = df_satisfied.reindex(index=df_predicted.index, columns=common_columns, fill_value=0)\n",
    "    \n",
    "    # Subtract the satisfied demand from the predicted demand\n",
    "    result_df = df_predicted - df_satisfied\n",
    "\n",
    "    # Ensure no negative values\n",
    "    result_df = result_df.clip(lower=0)\n",
    "    # als unsatisfied (predictions waarvoor nog geen d demand is) groter is dan opgegeven volume - volume d satisfied, scale die row predictions in results_df zodat het gelijk word\n",
    "    dict_unsatisfied = result_df.to_dict(orient='index')\n",
    "    for key in dict_unsatisfied:\n",
    "        volume_satisfied = 0\n",
    "        if key in satisfied_dict:\n",
    "            volume_satisfied = sum([satisfied_dict[key][region] for region in satisfied_dict[key] if region != 'LAAD_CPC'])\n",
    "        volume_unsatisfied = sum([dict_unsatisfied[key][region] for region in dict_unsatisfied[key] if region != 'LAAD_CPC'])\n",
    "        volume_given = 0\n",
    "        if key in known_a_dict:\n",
    "            volume_given = known_a_dict[key]['PALLETPLAATSEN']\n",
    "        # print(f\"Volume unsatisfied: {volume_unsatisfied}, Volume a given: {volume_given}, Volume d satisfied: {volume_satisfied}\")\n",
    "        if volume_unsatisfied > volume_given - volume_satisfied:\n",
    "            scale_factor = min(1, (volume_given - volume_satisfied) / volume_unsatisfied)\n",
    "            # List of columns to scale, exclude 'LAAD_CPC'\n",
    "            columns_to_scale = [col for col in result_df.columns if col != 'LAAD_CPC' and col.startswith('REGION')]\n",
    "            if key in result_df.index:\n",
    "                # Apply scaling directly to the specific row for the specified columns\n",
    "                result_df.loc[key, columns_to_scale] *= scale_factor\n",
    "                \n",
    "                # Now, apply floor to round down the values in the scaled columns\n",
    "                result_df.loc[key, columns_to_scale] = np.round(result_df.loc[key, columns_to_scale])\n",
    "\n",
    "\n",
    "    # Add 'LAAD_CPC' back to result_df\n",
    "    result_df['LAAD_CPC'] = laad_cpc\n",
    "\n",
    "    # Reset index if you want 'MATCHING_KEY' back as a column\n",
    "    result_df.reset_index(inplace=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def compensate_predicted_volumes(df_predicted, df_satisfied, known_orders_a):\n",
    "    warnings.filterwarnings(\n",
    "    action='ignore', \n",
    "    message=\"Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas.\"\n",
    ")\n",
    "    if df_predicted.index.name != 'MATCHING_KEY':\n",
    "        df_predicted = df_predicted.set_index('MATCHING_KEY')\n",
    "    if df_satisfied.index.name != 'MATCHING_KEY':\n",
    "        df_satisfied = df_satisfied.set_index('MATCHING_KEY')\n",
    "    df_predicted['total_predicted'] = df_predicted[[column for column in df_predicted.columns if \"REGION\" in column]].sum(axis=1)\n",
    "    df_satisfied['total_satisfied'] = df_satisfied[[column for column in df_satisfied.columns if \"REGION\" in column]].sum(axis=1)\n",
    "\n",
    "\n",
    "    # Step 1: Make a copy of df_predicted to result_df\n",
    "    temp_df = df_predicted - df_satisfied\n",
    "    result_df = df_predicted.copy()\n",
    "\n",
    "\n",
    "    common_columns = df_satisfied.columns.intersection(result_df.columns)\n",
    "    common_rows = df_satisfied.index.intersection(result_df.index)\n",
    "\n",
    "    subtraction_result = temp_df.loc[common_rows, common_columns]\n",
    "    subtraction_result = subtraction_result.astype('float64')\n",
    "\n",
    "    # Step 4: Update result_df with the subtraction results\n",
    "    result_df.loc[common_rows, common_columns] = subtraction_result\n",
    "\n",
    "\n",
    "    result_df[common_columns] = result_df[common_columns].clip(lower=0)\n",
    "    result_df['total_satisfied'] = df_satisfied['total_satisfied']\n",
    "    # result_df['total_predicted'] = result_df.sum(axis=1)\n",
    "    result_df[\"should_predicted\"] = (result_df['total_predicted'] - result_df['total_satisfied']).clip(lower=0)\n",
    "    result_df[\"scaling_factor\"] = result_df[\"should_predicted\"] / result_df[\"total_predicted\"]\n",
    "    result_df[\"scaling_factor\"] = result_df[\"scaling_factor\"].fillna(1)\n",
    "    result_df[common_columns] = result_df[common_columns].multiply(result_df[\"scaling_factor\"], axis=0)\n",
    "    result_df.drop(columns=[\"total_predicted\", \"total_satisfied\", \"should_predicted\", \"scaling_factor\"], inplace=True)\n",
    "    result_df.reset_index(inplace=True)\n",
    "    result_df[common_columns] = np.round(result_df[common_columns])\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "def write_to_csv(df, file_name):\n",
    "    path = #filepath heree\n",
    "    \n",
    "    file_name = path + file_name\n",
    "    df.to_csv(file_name)\n",
    "\n",
    "def export_scenario(file_name, df_scenario):\n",
    "    df_scenario = df_scenario[df_scenario[\"PALLETPLAATSEN\"] < 32]\n",
    "    columns_to_keep = [\n",
    "                \"SHIPMENTNUMBER\", \"STATUS\", \"CREATIONDATETIME\", \"AFHCODE\", \"OPDRACHTGEVERID\", \"OPDRACHTGEVERNAAM\",\n",
    "                \"LAADZOEK\", \"LAADADRES\", \"LAADPLAATS\", \"LAADPC\", \"LAADLAND\", \"LAAD_DATETIME_VAN\", \"LAAD_DATETIME_TOT\",\n",
    "                \"LAADPLANK\", \"LOSZOEK\", \"LOSADRES\", \"LOSPLAATS\", \"LOSPC\", \"LOSLAND\", \"LOS_DATETIME_VAN\", \"LOS_DATETIME_TOT\",\n",
    "                \"LOSPLANK\", \"COLLIAANTAL\", \"COLLICODE\", \"PALLETPLAATSEN\", \"LAADRIT\", \"LOSRIT\", \"HAS_PICKUP_TRIP\", \n",
    "                \"HAS_DELIVERY_TRIP\", \"15CREATIONDATETIME\", \"LAAD_CPC\", \"LOS_CPC\"\n",
    "            ]\n",
    "    group_by_columns = ['LAAD_CPC', 'OPDRACHTGEVERID', 'LOS_CPC']\n",
    "    agg_dict = {col: 'last' for col in columns_to_keep if col not in group_by_columns and col != 'PALLETPLAATSEN' and col != 'SHIPMENTNUMBER'}\n",
    "    agg_dict.update({\n",
    "        'PALLETPLAATSEN': 'sum',\n",
    "        'SHIPMENTNUMBER': lambda x: list(x),\n",
    "        'COUNTER': 'count'  # Add a count for the number of orders\n",
    "    })\n",
    "    df_scenario = df_scenario.copy()\n",
    "    df_scenario.loc[:, \"COUNTER\"] = df_scenario.loc[:, \"SHIPMENTNUMBER\"].copy()\n",
    "    df_grouped = df_scenario.groupby(group_by_columns).agg(agg_dict).reset_index()\n",
    "    df_grouped.rename(columns={'COUNTER': 'ORDER_COUNT'}, inplace=True)\n",
    "    df_grouped = df_grouped[df_grouped[\"PALLETPLAATSEN\"] < 32]\n",
    "    write_to_csv(df_grouped, file_name)\n",
    "    return file_name\n",
    "\n",
    "def add_matching_key(df):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    df[\"MATCHING_KEY\"] = df['OPDRACHTGEVERID'].astype(str) + '_' + df['LAAD_DATETIME_VAN'].dt.date.astype(str) + '_' + df['LAADPC'].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "date = #sample date here\n",
    "planning_date = #Planning date here\n",
    "\n",
    "# times = [(datetime.strptime(\"10:00\", \"%H:%M\") + i * timedelta(minutes=15)).strftime(\"%H:%M\") \n",
    "#          for i in range((datetime.strptime(\"18:00\", \"%H:%M\") - datetime.strptime(\"10:00\", \"%H:%M\")) // timedelta(minutes=15) + 1)]\n",
    "# times = [\"12:00\"] \n",
    "# times = [\"6:00\", \"7:00\", \"8:00\", \"9:00\", \"10:00\", \"11:00\", \"12:00\", \"13:00\", \"14:00\", \"15:00\", \"16:00\", \"17:00\", \"18:00\", \"19:00\", \"20:00\"] \n",
    "times = [\"10:00\", \"11:00\", \"12:00\", \"13:00\", \"14:00\", \"15:00\", \"16:00\", \"17:00\", \"18:00\", \"19:00\"] \n",
    "n_scenarios = 20  \n",
    "\n",
    "print_bool = False\n",
    "\n",
    "df_orders_d = df_orders[df_orders[\"AFHCODE\"] == 'd']\n",
    "df_orders_a = df_orders[df_orders[\"AFHCODE\"] == 'a']\n",
    "time_list = []\n",
    "simulated_list = []\n",
    "simulated_volume_list = []\n",
    "known_list = []\n",
    "known_volume_list = []\n",
    "known_a_volume_list = []\n",
    "simulated_known_list = []\n",
    "simulated_known_volume_list = []\n",
    "total_a_satisfied_by_d_orders = []\n",
    "\n",
    "\n",
    "list_total_d_orders = []\n",
    "list_total_a_orders = []\n",
    "list_total_unpredictable_a_orders = []\n",
    "list_total_predictable_a_orders = []\n",
    "list_total_a_orders_satisfied_by_d_orders = []\n",
    "list_total_a_orders_not_satisfied_by_d_orders  = []\n",
    "list_total_a_orders_predicted  = []\n",
    "list_total_simulated_orders_after_correction = []\n",
    "list_total_simulated_orders_total = []\n",
    "\n",
    "names = []\n",
    "for time in times:\n",
    "    number = 0\n",
    "    for x in range(n_scenarios):\n",
    "        file_name = date + \"_\" + time.replace(\":\", \"\") + \"_DumbScenario_\" + str(number) + \".csv\"\n",
    "        df_simulated_orders = pd.DataFrame()\n",
    "    # for time in [\"8:00\"]:\n",
    "        known_orders_a, known_orders_d = SplitDFtoAandDbyTime(df_orders, date, planning_date, time)\n",
    "        if x == 0:\n",
    "            df_scenario = known_orders_d\n",
    "            # export_scenario(file_name ,df_scenario)\n",
    "            number += 1\n",
    "            continue\n",
    "        \n",
    "        pallets_satisfied = 0\n",
    "        pallets_predicted = 0\n",
    "        pallets_unsatisfied = 0\n",
    "        total_unsatisfied = 0\n",
    "        total_satisfied_by_d_orders = 0\n",
    "\n",
    "        \n",
    "\n",
    "        if print_bool:\n",
    "            print(f\"Total d orders:                                 {known_orders_d['PALLETPLAATSEN'].sum()}\")\n",
    "            print(\"                                          ------------------------------------\")\n",
    "            print(f\"Total a orders:                                 { known_orders_a['PALLETPLAATSEN'].sum()}\")\n",
    "        total_unpredictable_a_orders = 0\n",
    "        total_predictable_a_orders = 0\n",
    "        total_a_orders_satisfied_by_d_orders = 0\n",
    "        total_a_orders_not_satisfied_by_d_orders = 0\n",
    "        total_a_orders_predicted = 0\n",
    "        total_predicted_orders_after_correction = 0\n",
    "        total_a_orders_unsatisfied = 0\n",
    "\n",
    "        shipment_number = 5000000000\n",
    "        # for company in ['BOSCH_THERMOTECHNIEK_BV']:\n",
    "        for company in known_orders_a[\"OPDRACHTGEVERNAAM\"].unique():\n",
    "            df_company_scenario = pd.DataFrame()\n",
    "            df_predicted = 0\n",
    "            total_unpredictable_a_orders = known_orders_a[~known_orders_a[\"OPDRACHTGEVERNAAM\"].isin(predictor.GSE_list)][\"PALLETPLAATSEN\"].sum()\n",
    "            total_predictable_a_orders = known_orders_a[known_orders_a[\"OPDRACHTGEVERNAAM\"].isin(predictor.GSE_list)][\"PALLETPLAATSEN\"].sum()\n",
    "            if company in predictor.GSE_list:\n",
    "                \n",
    "                add_bool = False\n",
    "                df_satisfied = predictor.get_satisfied_demand_for_company(known_orders_d, company)\n",
    "                pallets_satisfied += sum([df_satisfied[region].sum() for region in df_satisfied.columns if region != 'MATCHING_KEY'])\n",
    "                #df_predicted = predictor.generate_predicted_orders(known_orders_a, company) #for predicted orders\n",
    "                df_predicted = predictor.generate_mean_orders(known_orders_a, company) # For mean orders\n",
    "                \n",
    "                total_a_orders_predicted += sum([df_predicted[region].sum() for region in df_predicted.columns if region != 'MATCHING_KEY' if region != 'LAAD_CPC'])\n",
    "\n",
    "                df_orders_company = add_matching_key(pd.concat([known_orders_a[known_orders_a['OPDRACHTGEVERNAAM'] == company], known_orders_d[known_orders_d['OPDRACHTGEVERNAAM'] == company]]))\n",
    "                \n",
    "                condensed_orders = predictor.GSE_list[company].condense_orders(df=df_orders_company)\n",
    "                total_a_orders_satisfied_by_d_orders += np.minimum(condensed_orders['PALLETPLAATSEN'], condensed_orders['PALLETPLAATSEN_ACTUAL']).sum()\n",
    "                total_a_orders_not_satisfied_by_d_orders += (condensed_orders['PALLETPLAATSEN'] - condensed_orders['PALLETPLAATSEN_ACTUAL']).clip(lower=0).sum()\n",
    "                \n",
    "                \n",
    "                df_unsatisfied = compensate_predicted_volumes(df_predicted, df_satisfied, known_orders_a)\n",
    "                pallets_unsatisfied += sum([df_unsatisfied[region].sum() for region in df_unsatisfied.columns if region != 'MATCHING_KEY' if region != 'LAAD_CPC'])\n",
    "                # Generate scenario rows for unsatisfied demand\n",
    "                \n",
    "                for _, row in df_unsatisfied.iterrows():\n",
    "                    temp_dict = row.to_dict()\n",
    "                    for key in temp_dict.keys():\n",
    "                        id = temp_dict[\"MATCHING_KEY\"].split(\"_\")[0]\n",
    "                        if key != \"MATCHING_KEY\" and key != \"LAAD_CPC\":\n",
    "                            add_bool = True\n",
    "                            while temp_dict[key] > 0:\n",
    "                                region_number = key.split(\"_\")[1]\n",
    "                                delivery_CPC = predictor.GSE_list[company].return_random_CPC_from_clusternr(int(region_number))\n",
    "                                date_obj = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "                                sample_order_size = predictor.GSE_list[company].sample_order_size_of_CPC(delivery_CPC)\n",
    "                                if sample_order_size > temp_dict[key]:\n",
    "                                    sample_order_size = temp_dict[key]\n",
    "                                temp_dict[key] -= sample_order_size\n",
    "                                shipment_number += 1\n",
    "                                \n",
    "                                data = {'LOS_CPC': delivery_CPC,\n",
    "                                        'LAAD_CPC': temp_dict[\"LAAD_CPC\"],\n",
    "                                        'OPDRACHTGEVERNAAM': company,\n",
    "                                        'OPDRACHTGEVERID': id,\n",
    "                                        'SHIPMENTNUMBER': shipment_number,\n",
    "                                        'PALLETPLAATSEN': sample_order_size,\n",
    "                                        'LOS_DATETIME_VAN': datetime.combine(date_obj, datetime.strptime(\"9:00\", \"%H:%M\").time()),\n",
    "                                        'LOS_DATETIME_TOT': datetime.combine(date_obj, datetime.strptime(\"17:00\", \"%H:%M\").time())}\n",
    "                                df_company_scenario = pd.concat([df_company_scenario, pd.DataFrame([data])])\n",
    "            df_simulated_orders = pd.concat([df_simulated_orders, df_company_scenario])\n",
    "        \n",
    "        total_simulated_orders_after_correction = df_simulated_orders['PALLETPLAATSEN'].sum()\n",
    "\n",
    "        df_scenario = pd.concat([df_simulated_orders, known_orders_d])\n",
    "        export_scenario(file_name ,df_scenario)\n",
    "        print(file_name)\n",
    "        number += 1\n",
    "        if print_bool:\n",
    "            print(f\"Total unpredictable a orders due to no model:   {total_unpredictable_a_orders}\")\n",
    "            print(\"                                          ------------------------------------\")\n",
    "            print(f\"Total predictable a orders:                     {total_predictable_a_orders}\")\n",
    "            \n",
    "            print(f\"Total a orders satisfied by d orders:           {total_a_orders_satisfied_by_d_orders}\")\n",
    "            print(\"                                          ------------------------------------\")\n",
    "            print(f\"Total a orders not satisfied by d orders:       {total_a_orders_not_satisfied_by_d_orders}\")\n",
    "            print(f\"Total a orders predicted:                       {total_a_orders_predicted}\")\n",
    "            print(f\"Total predicted orders after correction:        {total_simulated_orders_after_correction}\")\n",
    "            print(\"                                          ------------------------------------\")\n",
    "            print(f\"Total simulated orders:                         {df_scenario['PALLETPLAATSEN'].sum()}\")\n",
    "\n",
    "            # total_predicted_orders_after_correction = 0\n",
    "            # total_a_orders_unsatisfied = 0\n",
    "        list_total_d_orders.append(known_orders_d['PALLETPLAATSEN'].sum())\n",
    "        list_total_a_orders.append(total_predictable_a_orders) \n",
    "        list_total_unpredictable_a_orders.append(total_unpredictable_a_orders)\n",
    "        list_total_predictable_a_orders.append(total_predictable_a_orders)\n",
    "        list_total_a_orders_satisfied_by_d_orders.append(total_a_orders_satisfied_by_d_orders)\n",
    "        list_total_a_orders_not_satisfied_by_d_orders.append(total_a_orders_not_satisfied_by_d_orders)\n",
    "        list_total_a_orders_predicted.append(total_a_orders_predicted)\n",
    "        list_total_simulated_orders_after_correction.append(total_simulated_orders_after_correction)\n",
    "        list_total_simulated_orders_total.append(total_simulated_orders_after_correction + known_orders_d['PALLETPLAATSEN'].sum())\n",
    "\n",
    "\n",
    "        time_list.append(time)\n",
    "    # # simulated_list.append(len(df_simulated_orders))\n",
    "    # # simulated_volume_list.append(df_simulated_orders['PALLETPLAATSEN'].sum())\n",
    "    # # known_list.append(len(known_orders_d))\n",
    "    # # known_volume_list.append(known_orders_d['PALLETPLAATSEN'].sum())\n",
    "    # # simulated_known_list.append(len(df_scenario))\n",
    "    # # simulated_known_volume_list.append(df_scenario['PALLETPLAATSEN'].sum())\n",
    "    # # known_a_volume_list.append(known_orders_a['PALLETPLAATSEN'].sum())\n",
    "    # # total_a_satisfied_by_d_orders.append(total_satisfied_by_d_orders)\n",
    "\n",
    "    # # print(f\"Time: {time}\")\n",
    "    # # print(f\"Volume of simulated orders:         {df_simulated_orders['PALLETPLAATSEN'].sum()}\")\n",
    "    # # print(f\"Volume of known delivery orders:    {known_orders_d['PALLETPLAATSEN'].sum()}\")\n",
    "    # # print(f\"Volume of known a orders:           {known_orders_a['PALLETPLAATSEN'].sum()}\")\n",
    "    # # print(f\"Volume of simulated + kown orders:  {df_scenario['PALLETPLAATSEN'].sum()}\")\n",
    "    # # print(f\"Total a orders satisfied by d orders:        {total_satisfied_by_d_orders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results on a graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_list, list_total_simulated_orders_after_correction, label='Simulated Delivery Orders')\n",
    "plt.plot(time_list, list_total_d_orders, label='Known Delivery Orders')\n",
    "plt.plot(time_list, list_total_a_orders_satisfied_by_d_orders, label='Pickup Orders Satisfied by Delivery Orders')\n",
    "plt.plot(time_list, list_total_a_orders, label='Known Pickup Orders')\n",
    "plt.plot(time_list, list_total_simulated_orders_total, label='Simulated Delivery + Known Delivery Orders')\n",
    "\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.ylim(0, max(list_total_simulated_orders_total) + 100)\n",
    "plt.ylabel('Volume of Orders (Pallets)', fontsize=14)\n",
    "plt.title('Volume of Orders over Time', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# list_total_d_orders.append(known_orders_d['PALLETPLAATSEN'].sum())\n",
    "#     list_total_a_orders.append(total_predictable_a_orders) \n",
    "#     list_total_unpredictable_a_orders.append(total_unpredictable_a_orders)\n",
    "#     list_total_predictable_a_orders.append(total_predictable_a_orders)\n",
    "#     list_total_a_orders_satisfied_by_d_orders.append(total_a_orders_satisfied_by_d_orders)\n",
    "#     list_total_a_orders_not_satisfied_by_d_orders.append(total_a_orders_not_satisfied_by_d_orders)\n",
    "#     list_total_a_orders_predicted.append(total_a_orders_predicted)\n",
    "#     list_total_simulated_orders_after_correction.append(total_simulated_orders_after_correction)\n",
    "#     list_total_simulated_orders_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_list, list_total_simulated_orders_after_correction, label='Simulated Delivery Orders')\n",
    "plt.plot(time_list, list_total_d_orders, label='Known Delivery Orders')\n",
    "plt.plot(time_list, list_total_a_orders_satisfied_by_d_orders, label='Pickup Orders Satisfied by Delivery Orders')\n",
    "plt.plot(time_list, list_total_a_orders, label='Known Pickup Orders')\n",
    "plt.plot(time_list, list_total_simulated_orders_total, label='Simulated Delivery + Known Delivery Orders')\n",
    "\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.ylim(0, max(list_total_simulated_orders_total) + 100)\n",
    "plt.ylabel('Volume of Orders (Pallets)', fontsize=14)\n",
    "plt.title('Volume of Orders over Time', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "# Remove y-axis labels\n",
    "plt.gca().set_yticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the maximum value of list_total_d_orders\n",
    "max_value = max(list_total_d_orders)\n",
    "\n",
    "# Define y-ticks from 0% to 150% with a step of 10%\n",
    "yticks = np.arange(0, 111, 10)\n",
    "ytick_labels = [f'{y}%' for y in yticks]\n",
    "\n",
    "# Plot results on a graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_list, list_total_simulated_orders_after_correction, label='Simulated Delivery Orders')\n",
    "plt.plot(time_list, list_total_d_orders, label='Known Delivery Orders')\n",
    "plt.plot(time_list, list_total_a_orders_satisfied_by_d_orders, label='Pickup Orders Satisfied by Delivery Orders')\n",
    "plt.plot(time_list, list_total_a_orders, label='Known Pickup Orders')\n",
    "plt.plot(time_list, list_total_simulated_orders_total, label='Simulated Delivery + Known Delivery Orders')\n",
    "\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.ylabel('Percentage of Maximum Orders', fontsize=14)\n",
    "plt.title('Volume of Orders over Time', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "# Replace y-axis values with percentages and set custom ticks\n",
    "ax = plt.gca()\n",
    "ax.set_yticks(yticks / 100 * max_value)  # Set y-ticks at intervals corresponding to 10% steps beyond 100%\n",
    "ax.set_yticklabels(ytick_labels, fontsize=12)  # Set y-tick labels as percentages\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_total_simulated_orders_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results on a graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_list, list_total_d_orders, label='Known d orders')\n",
    "plt.plot(time_list, list_total_a_orders, label='Known a orders')\n",
    "\n",
    "plt.plot(time_list, list_total_a_orders_satisfied_by_d_orders, label='a orders satisfied by d orders')\n",
    "plt.plot(time_list, list_total_a_orders_not_satisfied_by_d_orders, label='a orders not satisfied by d orders')\n",
    "plt.plot(time_list, list_total_simulated_orders_after_correction, label='Simulated orders')\n",
    "plt.plot(time_list, list_total_simulated_orders_total, label='Simulated + Known orders')\n",
    "plt.xlabel('Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, max(list_total_simulated_orders_total) + 100)\n",
    "plt.ylabel('Volume of orders (pallets)')\n",
    "plt.title('Volume of orders over time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# list_total_d_orders.append(known_orders_d['PALLETPLAATSEN'].sum())\n",
    "#     list_total_a_orders.append(total_predictable_a_orders) \n",
    "#     list_total_unpredictable_a_orders.append(total_unpredictable_a_orders)\n",
    "#     list_total_predictable_a_orders.append(total_predictable_a_orders)\n",
    "#     list_total_a_orders_satisfied_by_d_orders.append(total_a_orders_satisfied_by_d_orders)\n",
    "#     list_total_a_orders_not_satisfied_by_d_orders.append(total_a_orders_not_satisfied_by_d_orders)\n",
    "#     list_total_a_orders_predicted.append(total_a_orders_predicted)\n",
    "#     .append(total_simulated_orders_after_correction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scenario.to_excel(\"scenario.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solver-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
