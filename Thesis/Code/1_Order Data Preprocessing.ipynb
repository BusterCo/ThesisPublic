{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd6831-0f39-4af9-b3ba-1823bc63b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "from datetime import time, timedelta\n",
    "import hashlib\n",
    "import os\n",
    "print(\"Libraries and classes loaded\")\n",
    "\n",
    "def quickplot(temp_df, title):\n",
    "    temp_df['LAADDATUM'] = pd.to_datetime(temp_df['LAADDATUM'], format='mixed')\n",
    "    temp_df = temp_df[temp_df['LAADDATUM'].dt.year > 2021]\n",
    "    temp_df['LAADDATUM'] = temp_df['LAADDATUM'].dt.date\n",
    "    temp_df = temp_df[temp_df['PALLETPLAATSEN'] < 100000]\n",
    "    grouped_df = temp_df.groupby(['LAADDATUM'])['PALLETPLAATSEN'].sum().reset_index()\n",
    "\n",
    "    # Calculate the total PALLETPLAATSEN per LAADDATUM\n",
    "    total_per_day = grouped_df.groupby('LAADDATUM')['PALLETPLAATSEN'].sum()\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    total_per_day.plot(figsize=(15, 8), marker='o', linestyle='')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Total PALLETPLAATSEN')\n",
    "    plt.xlabel('LAADDATUM')\n",
    "    output_dir = 'plaatjes_temp'\n",
    "    file_path = os.path.join(output_dir, title + \".png\")\n",
    "    plt.savefig(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d0a0c",
   "metadata": {},
   "source": [
    "# Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039f70c-1cf8-4533-b8ff-d2671b02b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = os.getcwd()\n",
    "file_path = direct + \"\" # input data path here \n",
    "total_rows = sum(1 for row in open(file_path, 'r', encoding='utf-8'))\n",
    "chunk_size = 10000  \n",
    "tqdm.pandas(desc=\"Reading CSV\")\n",
    "chunks = pd.read_csv(file_path, chunksize=chunk_size, iterator=True)\n",
    "\n",
    "def print_df_size(df, text):\n",
    "    print(text + \": \", len(df))\t\n",
    "    #quickplot(df, text)\n",
    "\n",
    "df_orders = pd.concat(tqdm(chunks, total=total_rows//chunk_size))\n",
    "print_df_size(df_orders, \"Initial size\")\n",
    "\n",
    "#only take into account finished orders\n",
    "df_orders = df_orders[df_orders['STATUS'] <= 990]\n",
    "print_df_size(df_orders, \"After filtering on status\")\n",
    "\n",
    "# List of columns to check for NaN values\n",
    "columns_to_check = ['LAADPC',  'LOSPC',  \"LOSDATUM\", \"LAADDATUM\", \"CREATIONDATECHECK\", \"CREATIONTIMECHECK\", 'LOSTIJDTOT', 'LOSTIJDVAN','LAADTIJDVAN', 'LAADTIJDTOT',\n",
    "                    'PALLETPLAATSEN', \"LOSPLAATS\", \"SHIPMENTNUMBER\", \"STATUS\", \"COLLICODE\"] # \n",
    "df_orders = df_orders.dropna(subset=columns_to_check)\n",
    "print_df_size(df_orders, \"After dropping NaN values\")\n",
    "\n",
    "#Fix formats of various strings (setting 0 if NaN for losritten)\n",
    "columns_to_convert = [\"AFHCODE\", \"OPDRACHTGEVERNAAM\",  \"LAADZOEK\", \"LAADADRES\", \"LAADPLAATS\", \"LAADPC\", \"LAADLAND\", \"LOSZOEK\", \"LOSADRES\", \"LOSPLAATS\", \"LOSPC\", \"LOSLAND\"]\n",
    "for item in columns_to_convert:\n",
    "    df_orders[item] =df_orders[item].astype(str)\n",
    "df_orders['LOSPC'] = df_orders['LOSPC'].astype(str).str.replace(' ', '', regex=False)\n",
    "df_orders['LAADPC'] = df_orders['LAADPC'].astype(str).str.replace(' ', '', regex=False)\n",
    "\n",
    "#Fix formats of various numbers (setting 0 if NaN for losritten)\n",
    "df_orders[df_orders[\"LAADPLANKMK\"] == \"VTVB\"] = 0\n",
    "df_orders[df_orders[\"LOSPLANKMK\"] == \"VTVB\"] = 0\n",
    "columns_to_convert = [\"SHIPMENTNUMBER\", \"STATUS\", \"OPDRACHTGEVERID\", \"LAADRIT\", \"LOSRIT\", \"LAADPLANKMK\", \"LOSPLANKMK\", \"LAADRIT\", \"LOSRIT\"]\n",
    "for item in columns_to_convert:\n",
    "    df_orders[item] = df_orders[item].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "# Fix datetime formats\n",
    "df_orders['LOSDATUM'] = pd.to_datetime(df_orders['LOSDATUM'], format='mixed')\n",
    "df_orders['LAADDATUM'] = pd.to_datetime(df_orders['LAADDATUM'], format='mixed')\n",
    "df_orders['LAADTIJDVAN'] = pd.to_datetime(df_orders['LAADTIJDVAN'], format='mixed').dt.time\n",
    "df_orders['LAADTIJDTOT'] = pd.to_datetime(df_orders['LAADTIJDTOT'], format='mixed').dt.time\n",
    "df_orders['LOSTIJDVAN'] = pd.to_datetime(df_orders['LOSTIJDVAN'], format='mixed').dt.time\n",
    "df_orders['LOSTIJDTOT'] = pd.to_datetime(df_orders['LOSTIJDTOT'], format='mixed').dt.time\n",
    "df_orders['CREATIONDATECHECK'] = pd.to_datetime(df_orders['CREATIONDATECHECK'], format='mixed')\n",
    "df_orders['CREATIONTIMECHECK'] = pd.to_datetime(df_orders['CREATIONTIMECHECK'], format='mixed').dt.time\n",
    "\n",
    "\n",
    "# Convert 'CREATIONDATECHECK' to date format and 'CREATIONTIMECHECK' to time format if they are not already\n",
    "df_orders['CREATIONDATECHECK'] = pd.to_datetime(df_orders['CREATIONDATECHECK']).dt.date\n",
    "#df_orders['CREATIONTIMECHECK'] = pd.to_datetime(df_orders['CREATIONTIMECHECK'], format='%H:%M').dt.time\n",
    "\n",
    "# Combine the date and time columns into a new datetime column\n",
    "df_orders.loc[:, 'CREATION_DATETIME'] = pd.to_datetime(df_orders['CREATIONDATECHECK'].astype(str) + ' ' + df_orders['CREATIONTIMECHECK'].astype(str))\n",
    "\n",
    "df_orders.loc[:, 'LAAD_DATETIME_VAN'] = pd.to_datetime(df_orders['LAADDATUM'].astype(str) + ' ' + df_orders['LAADTIJDVAN'].astype(str))\n",
    "df_orders.loc[:, 'LAAD_DATETIME_TOT'] = pd.to_datetime(df_orders['LAADDATUM'].astype(str) + ' ' + df_orders['LAADTIJDTOT'].astype(str))\n",
    "df_orders.loc[:, 'LOS_DATETIME_VAN'] = pd.to_datetime(df_orders['LOSDATUM'].astype(str) + ' ' + df_orders['LOSTIJDVAN'].astype(str))\n",
    "df_orders.loc[:, 'LOS_DATETIME_TOT'] = pd.to_datetime(df_orders['LOSDATUM'].astype(str) + ' ' + df_orders['LOSTIJDTOT'].astype(str))\n",
    "\n",
    "print(\"fixed time\")\n",
    "\n",
    "\n",
    "rename_mapping = {0: 'NORMAL TRAILER', 23: 'BOXTRUCK SMALL', 24: 'CITY TRAILER', 25: 'NORMAL TRAILER'}\n",
    "df_orders['LAADPLANKMK_renamed'] = df_orders['LAADPLANKMK'].map(rename_mapping)\n",
    "df_orders['LOSPLANKMK_renamed'] = df_orders['LOSPLANKMK'].map(rename_mapping)\n",
    "print(\"fixed loading type\")\n",
    "\n",
    "# Drop duplicates keeping the first entry, which is the latest due to the sort order\n",
    "df_orders = df_orders.sort_values(by=['SHIPMENTNUMBER', 'CREATIONDATECHECK'], ascending=[True, False])\n",
    "df_orders = df_orders.drop_duplicates(subset='SHIPMENTNUMBER', keep='first').reset_index(drop=True)\n",
    "print_df_size(df_orders, \"After dropping duplicates\")\n",
    "\n",
    "\n",
    "# Filter out rows where the first character of 'LAADPC' or 'LOSPC' is not a digit\n",
    "df_orders = df_orders[df_orders.apply(lambda row: \n",
    "                                      (isinstance(row['LAADPC'], str) and row['LAADPC'] and row['LAADPC'][0].isdigit()) and \n",
    "                                      (isinstance(row['LOSPC'], str) and row['LOSPC'] and row['LOSPC'][0].isdigit()), \n",
    "                                      axis=1)]\n",
    "print_df_size(df_orders, \"After removing invalid postal codes\")\n",
    "\n",
    "# Remove rows where 'PALLETPLAATSEN' is greater than 100\n",
    "df_orders = df_orders[df_orders['PALLETPLAATSEN'] <= 100]\n",
    "print_df_size(df_orders, \"After removing rows with more than 100 pallet places\")\n",
    "\n",
    "df_orders = df_orders[df_orders[\"LAADDATUM\"].dt.year > 2021]\n",
    "df_orders = df_orders[df_orders[\"LAADDATUM\"].dt.year < 2024]\n",
    "print_df_size(df_orders, \"After removing rows with year before 2022\")\n",
    "\n",
    "\n",
    "\n",
    "# Clean suplier names\n",
    "replacement_pattern = r'[.,!&\\'()/]'  # List of characters to replace\n",
    "df_orders = df_orders[~df_orders['OPDRACHTGEVERNAAM'].str.contains('INSERT_COMPANIES_TO_EXCLUDE_HERE', case=False)] \n",
    "df_orders[\"OPDRACHTGEVERNAAMCLEAN\"] = df_orders['OPDRACHTGEVERNAAM'].str.replace(replacement_pattern, '', regex=True).str.replace(' ', '_', regex=False).str.replace('&', '_', regex=False).str.replace('Ã‹', 'E', regex=False)\n",
    "print_df_size(df_orders, \"After removing certain customers\")\n",
    "\n",
    "# Merge some supplyer names\n",
    "rename_dict = {}\n",
    "\n",
    "# Apply the renaming operation for each key in the rename_dict\n",
    "for new_name, old_names in rename_dict.items():\n",
    "    # Create a regex pattern from the list of old names\n",
    "    pattern = '|'.join(old_names)\n",
    "\n",
    "    # Replace occurrences of the old names with the new name\n",
    "    df_orders.loc[df_orders['OPDRACHTGEVERNAAMCLEAN'].str.contains(pattern, case=False, na=False), 'OPDRACHTGEVERNAAMCLEAN'] = new_name\n",
    "\n",
    "## Create unique IDs\n",
    "opdrachtgever_dict = {}\n",
    "def generate_unique_id(row):\n",
    "    if row[\"OPDRACHTGEVERNAAMCLEAN\"] in opdrachtgever_dict:\n",
    "        return opdrachtgever_dict[row[\"OPDRACHTGEVERNAAMCLEAN\"]]\n",
    "    else:\n",
    "        hash_object = hashlib.sha256(row['OPDRACHTGEVERNAAMCLEAN'].encode('utf-8'))\n",
    "        # Get the first 8 digits from the hash (as numbers)\n",
    "        unique_id = int(hash_object.hexdigest()[:8], 16)\n",
    "        \n",
    "        opdrachtgever_dict[row['OPDRACHTGEVERNAAMCLEAN']] = unique_id\n",
    "        return unique_id\n",
    "\n",
    "# Apply the function to generate opdrachtgeverids\n",
    "df_orders['OPDRACHTGEVERID'] = df_orders.apply(generate_unique_id, axis=1)\n",
    "\n",
    "# Unify COLLICODE column \n",
    "df_orders['COLLICODE']\t= df_orders['COLLICODE'].str.upper()\n",
    "\n",
    "\n",
    "print(\"Done, final lenght:\", len(df_orders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84051009-5962-4e60-ae9d-bc6ed62ca130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_export = pd.DataFrame()\n",
    "\n",
    "df_orders_export['SHIPMENTNUMBER'] = df_orders['SHIPMENTNUMBER']\n",
    "df_orders_export['STATUS'] = df_orders['STATUS']\n",
    "df_orders_export['CREATIONDATETIME'] = df_orders['CREATION_DATETIME']\n",
    "df_orders_export['AFHCODE'] = df_orders['AFHCODE']\n",
    "df_orders_export['OPDRACHTGEVERID'] = df_orders['OPDRACHTGEVERID']\n",
    "df_orders_export['OPDRACHTGEVERNAAM'] = df_orders['OPDRACHTGEVERNAAMCLEAN']\n",
    "\n",
    "df_orders_export['LAADZOEK'] = df_orders['LAADZOEK']\n",
    "df_orders_export['LAADADRES'] = df_orders['LAADADRES']\n",
    "df_orders_export['LAADPLAATS'] = df_orders['LAADPLAATS']\n",
    "df_orders_export['LAADPC'] = df_orders['LAADPC']\n",
    "df_orders_export['LAADLAND'] = df_orders['LAADLAND']\n",
    "df_orders_export['LAAD_DATETIME_VAN'] = df_orders['LAAD_DATETIME_VAN']\n",
    "df_orders_export['LAAD_DATETIME_TOT'] = df_orders['LAAD_DATETIME_TOT']\n",
    "df_orders_export['LAADPLANK'] = df_orders['LAADPLANKMK_renamed']\n",
    "\n",
    "df_orders_export['LOSZOEK'] = df_orders['LOSZOEK']\n",
    "df_orders_export['LOSADRES'] = df_orders['LOSADRES']\n",
    "df_orders_export['LOSPLAATS'] = df_orders['LOSPLAATS']\n",
    "df_orders_export['LOSPC'] = df_orders['LOSPC']\n",
    "df_orders_export['LOSLAND'] = df_orders['LOSLAND']\n",
    "df_orders_export['LOS_DATETIME_VAN'] = df_orders['LOS_DATETIME_VAN']\n",
    "df_orders_export['LOS_DATETIME_TOT'] = df_orders['LOS_DATETIME_TOT']\n",
    "df_orders_export['LOSPLANK'] = df_orders['LOSPLANKMK_renamed']\n",
    "\n",
    "df_orders_export['COLLIAANTAL'] = df_orders['COLLIAANTAL']\n",
    "df_orders_export['COLLICODE'] = df_orders['COLLICODE']\n",
    "df_orders_export['PALLETPLAATSEN'] = df_orders['PALLETPLAATSEN']\n",
    "df_orders_export['LAADRIT'] = df_orders['LAADRIT']\n",
    "df_orders_export['LOSRIT'] = df_orders['LOSRIT']\n",
    "df_orders_export[\"HAS_PICKUP_TRIP\"] = df_orders['LAADRIT'].astype(bool)\n",
    "df_orders_export[\"HAS_DELIVERY_TRIP\"] = df_orders['LOSRIT'].astype(bool)\n",
    "df_orders_export = df_orders_export[df_orders_export[\"STATUS\"] >= 460]\n",
    "df_orders_export = df_orders_export[df_orders_export[\"HAS_PICKUP_TRIP\"] | df_orders_export[\"HAS_DELIVERY_TRIP\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54199564",
   "metadata": {},
   "source": [
    "### Load status time changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46379625",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = os.getcwd()\n",
    "file_path = direct + \"\" # STATUS DATASET HERE\n",
    "total_rows = sum(1 for row in open(file_path, 'r', encoding='utf-8'))\n",
    "chunk_size = 10000  \n",
    "tqdm.pandas(desc=\"Reading CSV\")\n",
    "chunks = pd.read_csv(file_path, chunksize=chunk_size, iterator=True)\n",
    "\n",
    "df_loaded = pd.concat(tqdm(chunks, total=total_rows//chunk_size))\n",
    "df_statustimes = pd.DataFrame()\n",
    "\n",
    "df_statustimes[\"SHIPMENTNUMBER\"] = df_loaded['dosvlg']\n",
    "df_statustimes[\"DATETIME\"] = pd.to_datetime(df_loaded['tscrdz'], format='%d-%b-%y %I.%M.%S.%f %p %Z')\n",
    "df_statustimes[\"STATUSCHANGE\"] = df_loaded['tsrm68']\n",
    "df_statustimes['DATETIME'] = df_statustimes['DATETIME'].dt.tz_localize(None) #hier of localize of convert gebruiken, checken wat klopt.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lenght of input data:\", str(len(df_statustimes)))\n",
    "status_changes = ['  0 -> 15', ' 12 -> 15',' 13 -> 15', ' 10 -> 15']\n",
    "df_statustimes_filtered = df_statustimes[df_statustimes['STATUSCHANGE'].isin(status_changes)]\n",
    "\n",
    "# Sort and drop duplicates\n",
    "df_statustimes_filtered = df_statustimes_filtered.sort_values('DATETIME')\n",
    "df_statustimes_filtered = df_statustimes_filtered.drop_duplicates(subset=['SHIPMENTNUMBER'], keep='first')\n",
    "\n",
    "# Set index for merge\n",
    "df_statustimes_filtered.set_index('SHIPMENTNUMBER', inplace=True)\n",
    "\n",
    "\n",
    "df_statustimes_filtered[\"TIMEZONE\"] = df_statustimes_filtered[\"DATETIME\"].dt.tz\n",
    "df_statustimes_filtered[\"TIMEZONE\"].value_counts()\n",
    "\n",
    "# Extract and count timezone offsets\n",
    "df_statustimes_filtered['timezone_offset'] = df_statustimes_filtered['DATETIME'].dt.strftime('%z')\n",
    "offset_counts = df_statustimes_filtered['timezone_offset'].value_counts()\n",
    "\n",
    "print(\"Lenght status times input data after filter:\", str(len(df_statustimes_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e5c2c",
   "metadata": {},
   "source": [
    "### Merge statustimes with initial dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_export_status = df_orders_export\n",
    "# Merge and rename the column\n",
    "df_orders_export_status = df_orders_export_status.join(df_statustimes_filtered['DATETIME'], on='SHIPMENTNUMBER', how='left')\n",
    "df_orders_export_status.rename(columns={'DATETIME': '15CREATIONDATETIME'}, inplace=True)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "df_orders_export_status['15CREATIONDATETIME'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9e382",
   "metadata": {},
   "source": [
    "### Get locations for orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from custom.PC_Class import PC  # Import the module if you haven't already imported it separately\n",
    "from custom.GeoSpatialEncoder import GeoSpatialEncoder\n",
    "PC_obj = PC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88be12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_export_status[\"LAAD_CPC\"] = df_orders_export_status.apply(lambda row: PC_obj.return_CPC(row[\"LAADLAND\"], row[\"LAADPC\"]), axis=1);\n",
    "df_orders_export_status[\"LOS_CPC\"] = df_orders_export_status.apply(lambda row: PC_obj.return_CPC(row[\"LOSLAND\"], row[\"LOSPC\"]), axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f729307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ef30e5",
   "metadata": {},
   "source": [
    "Remove orders with invalid pickup or delivery PC's and without a creationtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0822e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_export_status = df_orders_export_status[~((df_orders_export_status[\"LAAD_CPC\"] == 0) | (df_orders_export_status[\"LOS_CPC\"] == 0))]\n",
    "df_orders = df_orders_export_status[(df_orders_export_status[\"15CREATIONDATETIME\"] != 0)].copy()\n",
    "len(df_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064794a",
   "metadata": {},
   "source": [
    "export this dataframe for scenario purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = os.getcwd()\n",
    "file_path = direct +  #OUTPUT PATH HERE\n",
    "df_orders.to_csv(file_path)\n",
    "print(\"Exported to \", file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608bcfbc",
   "metadata": {},
   "source": [
    "## Match orders (generate unique ID for each order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58199182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique key by concatenating the fields opdrachtgever with laaddatum and laadpc\n",
    "df_orders['MATCHING_KEY'] = df_orders['OPDRACHTGEVERID'].astype(str) + '_' + df_orders['LAAD_DATETIME_VAN'].dt.date.astype(str) + '_' + df_orders['LAADPC'].astype(str)\n",
    "a_type_keys = set(df_orders[df_orders['AFHCODE'] == 'a']['MATCHING_KEY'].unique())\n",
    "d_type_keys = set(df_orders[df_orders['AFHCODE'] == 'd']['MATCHING_KEY'].unique())\n",
    "f_type_keys = set(df_orders[df_orders['AFHCODE'] == 'f']['MATCHING_KEY'].unique())\n",
    "x_type_keys = set(df_orders[df_orders['AFHCODE'] == 'x']['MATCHING_KEY'].unique())\n",
    "r_type_keys = set(df_orders[df_orders['AFHCODE'] == 'r']['MATCHING_KEY'].unique())\n",
    "all_keys = (a_type_keys | d_type_keys | f_type_keys)\n",
    "shared_keys = (a_type_keys & (d_type_keys | f_type_keys))\n",
    "unshared_keys = (all_keys - shared_keys)\n",
    "\n",
    "df_orders['MATCHING_KEY'] = df_orders['MATCHING_KEY'].where(df_orders['MATCHING_KEY'].isin(a_type_keys), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b6b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ddaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A orders with no matching D or F orders\n",
    "df_a = df_orders[df_orders['AFHCODE'] == 'a']\n",
    "print(\"A orders:\", len(df_a))\n",
    "\n",
    "# These are orders that are not leading to deliveries (cancelled)\n",
    "df_a_without_d_f = df_orders[((df_orders['MATCHING_KEY'].isin(a_type_keys-shared_keys)) & (df_orders['AFHCODE'] == 'a'))]\n",
    "print(\"A orders without D or F key:\", len(df_a_without_d_f))\n",
    "\n",
    "# These are orders leading to deliveries\n",
    "df_a_with_d_f = df_orders[((df_orders['MATCHING_KEY'].isin(shared_keys)) & (df_orders['AFHCODE'] == 'a'))]\n",
    "print(\"A orders with D or F key:\", len(df_a_with_d_f))\n",
    "\n",
    "# These are the delivery orders of the a orders above\n",
    "df_df_with_a = df_orders[((df_orders['MATCHING_KEY'].isin(a_type_keys)) & ((df_orders['AFHCODE'] == 'd') | (df_orders['AFHCODE'] == 'f')))]\n",
    "print(\"DF orders with an A order:\", len(df_df_with_a))\n",
    "\n",
    "# Deliverd A and D F orders\n",
    "df_matched = df_orders[(df_orders['MATCHING_KEY'].isin(shared_keys) & ((df_orders['AFHCODE'] == 'a') |(df_orders['AFHCODE'] == 'd') | (df_orders['AFHCODE'] == 'f')))]\n",
    "df_matched['LAAD_DATE'] = df_matched['LAAD_DATETIME_VAN'].dt.date\n",
    "print(\"total orders with a matching a key:\", len(df_matched))\n",
    "print(\"total orders:\", len(df_orders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date from 'LAAD_DATETIME_VAN'\n",
    "\n",
    "\n",
    "def get_perfect_matches(df_imperfect_matches, df_perfect_matches, df_pickup, df_delivery, tolerance_percent, tolerance_pallets):\n",
    "    # Takes as input the total set of orders, the current perfect matches, and the pickup and delivery orders to match.\n",
    "\n",
    "    # Calculate pickup and delivery totals for the two inserted groups grouped on matching key\n",
    "    grouped_a = df_pickup.groupby('MATCHING_KEY')['PALLETPLAATSEN'].sum().reset_index()\n",
    "    grouped_d = df_delivery.groupby('MATCHING_KEY')['PALLETPLAATSEN'].sum().reset_index()\n",
    "    \n",
    "    # Get the difference of the totals of matching keys, and get the tolerance perfect matches\n",
    "    merged_totals = grouped_a.merge(grouped_d, on=['MATCHING_KEY'], how='outer', suffixes=('_A', '_D'))\n",
    "    merged_totals['Difference'] = merged_totals['PALLETPLAATSEN_D'] - merged_totals['PALLETPLAATSEN_A']\n",
    "    merged_totals['Difference_percent'] = merged_totals['Difference'].abs() / merged_totals['PALLETPLAATSEN_A']\n",
    "    perfect_matches = list(merged_totals[((merged_totals['Difference_percent'] < tolerance_percent) \n",
    "                                          | (merged_totals['Difference'].abs() < tolerance_pallets))][\"MATCHING_KEY\"])\n",
    "\n",
    "    temp_df = pd.concat([df_pickup[df_pickup['MATCHING_KEY'].isin(perfect_matches)], df_delivery[df_delivery['MATCHING_KEY'].isin(perfect_matches)]])\n",
    "    \n",
    "    df_perfect_matches = pd.concat([df_perfect_matches, temp_df])\n",
    "    df_imperfect_matches = df_imperfect_matches[~df_imperfect_matches[\"MATCHING_KEY\"].isin(perfect_matches)]\n",
    "    \n",
    "    \n",
    "    print(\"Number of matched keys this iteration: \", len(perfect_matches))\n",
    "    print(\"Total rows perfectly matched         : \", len(df_perfect_matches))\n",
    "    print(\"Total keys perfectly match           : \", len(df_perfect_matches[\"MATCHING_KEY\"].unique()))\n",
    "    print(\"Total rows remaining to match        : \", len(df_imperfect_matches))\n",
    "    print(\"Total keys remaining to match        : \", len(df_imperfect_matches[\"MATCHING_KEY\"].unique()))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return df_perfect_matches, df_imperfect_matches\n",
    "\n",
    "def filter_delivery_by_first_pickup(df_pickup, df_delivery):\n",
    "    # Calculate the minimum creation date for each matching key in one step\n",
    "    earliest_pickup_dates = df_pickup.groupby(\"MATCHING_KEY\")[\"CREATIONDATETIME\"].min()\n",
    "\n",
    "    # Use a map to apply the earliest pickup dates to the df_delivery DataFrame\n",
    "    df_delivery = df_delivery[\n",
    "        df_delivery[\"CREATIONDATETIME\"] >= df_delivery[\"MATCHING_KEY\"].map(earliest_pickup_dates)\n",
    "    ]\n",
    "\n",
    "    return df_delivery\n",
    "\n",
    "tolerance_percent = 0.3\n",
    "tolerance_pallets = 5\n",
    "\n",
    "df_imperfect_matches = df_matched\n",
    "df_perfect_matches = pd.DataFrame()\n",
    "\n",
    "status_options = [990]\n",
    "\n",
    "for delivery_options in [['d'], ['d', 'f']]:\n",
    "    print(\"Delivery types       : \", delivery_options)\n",
    "    df_filtered_afhaal = df_imperfect_matches[((df_imperfect_matches['AFHCODE'] == 'a') & (df_imperfect_matches['HAS_PICKUP_TRIP']))]\n",
    "    df_filtered_delivery = df_imperfect_matches[((df_imperfect_matches['AFHCODE'].isin(delivery_options)) & (df_imperfect_matches['HAS_DELIVERY_TRIP']))]\n",
    "    df_perfect_matches, df_imperfect_matches = get_perfect_matches(df_imperfect_matches, df_perfect_matches, df_filtered_afhaal, df_filtered_delivery, tolerance_percent, tolerance_pallets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf99cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total rows perfectly matched: \", len(df_perfect_matches), \"Total rows imperfectly match: \", len(df_imperfect_matches), \"Total rows: \", len(df_perfect_matches) + len(df_imperfect_matches))\t\n",
    "df_imperfect_matches[\"AFHCODE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collicode on both\n",
    "\n",
    "for colicode in df_imperfect_matches['COLLICODE'].unique():\n",
    "    for delivery_options in [['d', 'f'], ['d']]:\n",
    "        print(\"Collicode type both  : \", colicode)\n",
    "        print(\"Delivery types       : \", delivery_options)\n",
    "        df_filtered_afhaal = df_imperfect_matches[((df_imperfect_matches['AFHCODE'] == 'a') & \n",
    "                                                   (df_imperfect_matches['HAS_PICKUP_TRIP']) & \n",
    "                                                   (df_imperfect_matches['COLLICODE'] == colicode))]\n",
    "        df_filtered_delivery = df_imperfect_matches[((df_imperfect_matches['AFHCODE'].isin(delivery_options)) & \n",
    "                                                     (df_imperfect_matches['HAS_DELIVERY_TRIP']) & \n",
    "                                                     (df_imperfect_matches['COLLICODE'] == colicode))]\n",
    "        df_perfect_matches, df_imperfect_matches = get_perfect_matches(df_imperfect_matches, df_perfect_matches, df_filtered_afhaal, df_filtered_delivery, tolerance_percent, tolerance_pallets)\n",
    "\n",
    "\n",
    "# Collicode on delivery\n",
    "for colicode in df_imperfect_matches['COLLICODE'].unique():\n",
    "    for delivery_options in [['d', 'f'], ['d']]:\n",
    "        print(\"Collicode type delivery  : \", colicode)\n",
    "        print(\"Delivery types           : \", delivery_options)\n",
    "        df_filtered_afhaal = df_imperfect_matches[((df_imperfect_matches['AFHCODE'] == 'a') & \n",
    "                                                   (df_imperfect_matches['HAS_PICKUP_TRIP']))]\n",
    "        df_filtered_delivery = df_imperfect_matches[((df_imperfect_matches['AFHCODE'].isin(delivery_options)) & \n",
    "                                                     (df_imperfect_matches['HAS_DELIVERY_TRIP']) & \n",
    "                                                     (df_imperfect_matches['COLLICODE'] == colicode))]\n",
    "        df_perfect_matches, df_imperfect_matches = get_perfect_matches(df_imperfect_matches, df_perfect_matches, df_filtered_afhaal, df_filtered_delivery, tolerance_percent, tolerance_pallets)\n",
    "\n",
    "# Collicode on pickup\n",
    "for colicode in df_imperfect_matches['COLLICODE'].unique():\n",
    "    for delivery_options in [['d', 'f'], ['d']]:\n",
    "        print(\"Collicode type pickup    : \", colicode)\n",
    "        print(\"Delivery types           : \", delivery_options)\n",
    "        df_filtered_afhaal = df_imperfect_matches[((df_imperfect_matches['AFHCODE'] == 'a') & \n",
    "                                                   (df_imperfect_matches['HAS_PICKUP_TRIP']) & \n",
    "                                                   (df_imperfect_matches['COLLICODE'] == colicode))]\n",
    "        df_filtered_delivery = df_imperfect_matches[((df_imperfect_matches['AFHCODE'].isin(delivery_options)) & \n",
    "                                                     (df_imperfect_matches['HAS_DELIVERY_TRIP']))]\n",
    "        df_perfect_matches, df_imperfect_matches = get_perfect_matches(df_imperfect_matches, df_perfect_matches, df_filtered_afhaal, df_filtered_delivery, tolerance_percent, tolerance_pallets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ed05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with delivery LAADRIT not equal to a pickup LAADRIT rit from df_imperfect_matches\n",
    "a_laadritten = df_imperfect_matches[df_imperfect_matches[\"AFHCODE\"] == 'a'][\"LAADRIT\"].unique()\n",
    "d_laadritten_pre = df_imperfect_matches[df_imperfect_matches[\"AFHCODE\"] == 'd'][\"LAADRIT\"].unique()\n",
    "print('Wie dit leest trekt een rietbak rose')\n",
    "print(\"Total amount of pickup LAADRITTEN in current dataset : \", len(a_laadritten))\n",
    "print(\"Total amount of delivery LAADRITTEN pre removal      : \", len(d_laadritten_pre))\n",
    "\n",
    "# add 0 to array a_laadritten\n",
    "a_laadritten = np.append(a_laadritten, 0)\n",
    "mask = ~((df_imperfect_matches['AFHCODE'] == 'd') & (~df_imperfect_matches['LAADRIT'].isin(a_laadritten)))\n",
    "df_imperfect_matches = df_imperfect_matches[mask]\n",
    "\n",
    "d_laadritten_post = df_imperfect_matches[df_imperfect_matches[\"AFHCODE\"] == 'd'][\"LAADRIT\"].unique()\n",
    "print(\"Total amount of delivery LAADRITTEN post removal     : \", len(d_laadritten_post))\n",
    "print(\"\\n\")\n",
    "# Run matching alg again\n",
    "for delivery_options in [['d', 'f'], ['d']]:\n",
    "    print(\"Delivery types       : \", delivery_options)\n",
    "    df_filtered_afhaal = df_imperfect_matches[((df_imperfect_matches['AFHCODE'] == 'a') & (df_imperfect_matches['HAS_PICKUP_TRIP']))]\n",
    "    df_filtered_delivery = df_imperfect_matches[((df_imperfect_matches['AFHCODE'].isin(delivery_options)) & (df_imperfect_matches['HAS_DELIVERY_TRIP']) )]\n",
    "    df_perfect_matches, df_imperfect_matches = get_perfect_matches(df_imperfect_matches, df_perfect_matches, df_filtered_afhaal, df_filtered_delivery, tolerance_percent, tolerance_pallets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ec637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perfect_matches_2(df_imperfect_matches, df_perfect_matches, df_pickup, df_delivery, tolerance):\n",
    "    df_delivery = filter_delivery_by_first_pickup(df_pickup, df_delivery)\n",
    "\n",
    "    # Calculate pickup and delivery totals for the two inserted groups grouped on matching key\n",
    "    grouped_a = df_pickup.groupby('MATCHING_KEY')['PALLETPLAATSEN'].sum().reset_index()\n",
    "    grouped_d = df_delivery.groupby('MATCHING_KEY')['PALLETPLAATSEN'].sum().reset_index()\n",
    "    \n",
    "    # Get the difference of the totals of matching keys, and get the tolerance perfect matches\n",
    "    merged_totals = grouped_a.merge(grouped_d, on=['MATCHING_KEY'], how='outer', suffixes=('_A', '_D'))\n",
    "    merged_totals['Difference'] = merged_totals['PALLETPLAATSEN_D'] - merged_totals['PALLETPLAATSEN_A']\n",
    "    merged_totals['Difference_percent'] = merged_totals['Difference'].abs() / merged_totals['PALLETPLAATSEN_A']\n",
    "    perfect_matches = list(merged_totals[merged_totals['Difference_percent'] < tolerance][\"MATCHING_KEY\"])\n",
    "\n",
    "    temp_df = pd.concat([df_pickup[df_pickup['MATCHING_KEY'].isin(perfect_matches)], df_delivery[df_delivery['MATCHING_KEY'].isin(perfect_matches)]])\n",
    "    \n",
    "    df_perfect_matches = pd.concat([df_perfect_matches, temp_df])\n",
    "    df_imperfect_matches = df_imperfect_matches[~df_imperfect_matches[\"MATCHING_KEY\"].isin(perfect_matches)]\n",
    "    \n",
    "    \n",
    "    print(\"Number of matched keys this iteration: \", len(perfect_matches))\n",
    "    print(\"Total rows perfectly matched         : \", len(df_perfect_matches))\n",
    "    print(\"Total keys perfectly match           : \", len(df_perfect_matches[\"MATCHING_KEY\"].unique()))\n",
    "    print(\"Total rows remaining to match        : \", len(df_imperfect_matches))\n",
    "    print(\"Total keys remaining to match        : \", len(df_imperfect_matches[\"MATCHING_KEY\"].unique()))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return df_perfect_matches, df_imperfect_matches\n",
    "\n",
    "tolerance = 0.1\n",
    "# Run iteration where deliveries are created before pickups\n",
    "for delivery_options in [['d'], ['d', 'f']]:\n",
    "    print(\"Delivery types       : \", delivery_options)\n",
    "    df_filtered_afhaal = df_imperfect_matches[((df_imperfect_matches['AFHCODE'] == 'a') & (df_imperfect_matches['HAS_PICKUP_TRIP']))]\n",
    "    df_filtered_delivery = df_imperfect_matches[((df_imperfect_matches['AFHCODE'].isin(delivery_options)) & (df_imperfect_matches['HAS_DELIVERY_TRIP']) )]\n",
    "    df_perfect_matches, df_imperfect_matches = get_perfect_matches_2(df_imperfect_matches, df_perfect_matches, df_filtered_afhaal, df_filtered_delivery, tolerance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semi_perfect_matches = pd.DataFrame()\n",
    "\n",
    "top_volume_companies = list(df_orders[((df_orders[\"HAS_PICKUP_TRIP\"]) & (df_orders[\"AFHCODE\"] == 'a'))].groupby('OPDRACHTGEVERNAAM')['PALLETPLAATSEN'].sum().sort_values(ascending=False).index[:40])\n",
    "print(top_volume_companies)\n",
    "\n",
    "for opdrachtgever in top_volume_companies:\n",
    "    df_pickup = df_imperfect_matches[(df_imperfect_matches[\"OPDRACHTGEVERNAAM\"] == opdrachtgever) & \n",
    "                                    (df_imperfect_matches[\"HAS_PICKUP_TRIP\"]) &\n",
    "                                    (df_imperfect_matches[\"AFHCODE\"] == 'a')]\n",
    "    df_delivery = df_imperfect_matches[(df_imperfect_matches[\"OPDRACHTGEVERNAAM\"] == opdrachtgever) & \n",
    "                                    (df_imperfect_matches[\"HAS_DELIVERY_TRIP\"]) &\n",
    "                                    (df_imperfect_matches[\"AFHCODE\"] == 'd')]\n",
    "\n",
    "\n",
    "    # Calculate pickup and delivery totals for the two inserted groups grouped on matching key\n",
    "    grouped_a = df_pickup.groupby('MATCHING_KEY')['PALLETPLAATSEN'].sum().reset_index()\n",
    "    grouped_d = df_delivery.groupby('MATCHING_KEY')['PALLETPLAATSEN'].sum().reset_index()\n",
    "\n",
    "    # Get the difference of the totals of matching keys, and get the tolerance perfect matches\n",
    "    merged_totals = grouped_a.merge(grouped_d, on=['MATCHING_KEY'], how='outer', suffixes=('_A', '_D'))\n",
    "    merged_totals['Difference'] = merged_totals['PALLETPLAATSEN_D'] - merged_totals['PALLETPLAATSEN_A']\n",
    "    merged_totals['Difference_percent'] = merged_totals['PALLETPLAATSEN_D'] / merged_totals['PALLETPLAATSEN_A']\n",
    "    perfect_matches = list(merged_totals[(merged_totals['Difference_percent'] > tolerance) & ((merged_totals['Difference_percent'] < 5) | (merged_totals['PALLETPLAATSEN_A'] < 5))][\"MATCHING_KEY\"])\n",
    "    print(f\"Opdrachtgever {opdrachtgever}: {len(perfect_matches)}\")\n",
    "\n",
    "    temp_df = pd.concat([df_pickup[df_pickup['MATCHING_KEY'].isin(perfect_matches)], df_delivery[df_delivery['MATCHING_KEY'].isin(perfect_matches)]])\n",
    "\n",
    "    df_semi_perfect_matches = pd.concat([df_semi_perfect_matches, temp_df])\n",
    "    df_imperfect_matches = df_imperfect_matches[df_imperfect_matches[\"OPDRACHTGEVERNAAM\"] != opdrachtgever]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7f27fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_export = pd.concat([df_perfect_matches, df_semi_perfect_matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc1062",
   "metadata": {},
   "source": [
    "## Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beca00f-1c38-46d7-aafc-7132112dc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = os.getcwd()\n",
    "file_path = direct + \"\\\\data\\\\vos_input_data\\\\MultiHubData3_training.csv\" \n",
    "df_matches_export.to_csv(file_path)\n",
    "print(\"Exported to \", file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
